<!DOCtype html>
<html>
<head>
  <meta charset="utf-8">
  <title>Will Sankey | Data and code</title>
   <script src="//use.typekit.net/puo2mkm.js"></script>
	<script>try{Typekit.load({ async: true });}catch(e){}</script>
    <link rel=stylesheet type=text/css href="/blog/static/css/bootstrap.min.css"/>
  	<link rel=stylesheet type=text/css href="/blog/static/css/prism.css"/>
    <script src="//code.jquery.com/jquery-2.1.4.min.js"></script>
  	  <script src="/blog/static/js/prism.js"></script>
    <script src="//netdna.bootstrapcdn.com/bootstrap/3.3.2/js/bootstrap.min.js"></script>
    <link rel=stylesheet type=text/css href="/blog/static/css/styles.css"/>
    <link rel=stylesheet type=text/css href="/blog/static/css/fivepy.css"/>

</head>
<body>
<div class="container">
	<div class="row">
		<div class="col-sm-12">
  <h1><a href="/blog/">Will Sankey</a>
  <small>
  <span class="namegroup">
  <ul id="namelist">
    <li id="list"><a href="/blog/aboutwill/">[About Me, </a></li>
    <li id="list"><a href="/blog/posts/">Blog,</a></li>
    <li id="list"><a href="/blog/projectz/">Projects]</a></h3></li>
  </ul>
  </span>
  </small>
  </h1>


<div class="container">
	<div class="row">
		<div  class="col-xs-12 card" style="margin-bottom: 4em;">
					<a href="/blog/posts/"></a><span class="glyphicon glyphicon-hand-left"></span>
			<h2>DC Python Meetup - Hadoop with Python</h2>
			<p class="date">2015-03-12</p>
			<p>The following are my notes from the March DC Python meetup on Hadoop, given by Donald P. Miner (@donaldpminer). They might be incomplete. </p>
<h2>What is Hadoop</h2>
<p>This technology derives from Google and reimagines data storage. 
It mostly lives as a Hadoop Distributed File System (HDFS) which: </p>
<ul>
<li>Stores files in folders</li>
<li>Chunks large files into blocks</li>
<li>Duplicates the block three times</li>
<li>Scatters the blocks all over the place</li>
</ul>
<p>Can only create, read, and delete. If you want to edit you can use
something like HBase to run on top on Hadoop such as Acumulo, created by the NSA. The core piece of Hadoop that helps organize the data is MapReduce.</p>
<h2>MapReduce</h2>
<p>Analyzes raw data in HDFS. Jobs are split into Mappers and
Reducers. There are a lot of pieces that 'fall out' from mapping and
reducing. Donald Miner was able to write a book just on the different
things that can be done just with mapping and reducing. <a href="http://shop.oreilly.com/product/0636920025122.do">See MapReduce
design patterns.</a></p>
<h3>Mappers</h3>
<p>The key items of Mappers:</p>
<ul>
<li>Loads data form HDFS</li>
<li>Filter, transform, parse</li>
<li>Outputs (key, value) pairs</li>
</ul>
<h3>Reducers</h3>
<p>The key pieces of Reducers:</p>
<ul>
<li>Automatically groups by the mapper's output key</li>
<li>Aggregate, count, statistics</li>
<li>Outputs to HDFS</li>
</ul>
<p>MapReduce solves batching problems but is not a panacea. Other problems, such as
streaming data, are not appropriate for MapReduce.</p>
<p>Miner's personal workflow is to process in Hadoop and output to CSV or JSON.</p>
<h2>Hadoop Ecosystem</h2>
<p>Hadoop lives within an ecosystem of other projects:</p>
<ul>
<li>Higher level languages like Pig and Hive</li>
<li>HDFS data systems like HBase and Accumulo</li>
<li>Alternative executiion engines like Storm and Spark</li>
<li>Close friends like ZooKeeper, Flume, Avro, Kafka</li>
</ul>
<h3>The one thing I don't like about Hadoop is Java</h3>
<p>But Hadoop with Python is a bit half-baked.</p>
<h2>Cool things</h2>
<p>Data Locality - the idea of processing data where it is instead of
moving it over the network
<em> Linear Scalability
  * HDFS and MapReduce scale linearly
</em> Schema on Read (opposed to schema on write)
  * You load the data first, before you know what you're going to do
    with it. And then the nature of the data, whether it's JSON and what data types there are done later. 
  * You don't have to get rid of the original data!
Example: image analysis. You can have a mapper analyze the metadata
(e.g. who took it) and another to analyze the image. You can even store
it first and then do something with it later.
<em> Transparent Parallelism
  * You don't have to care about a lot of traditional worries like
    scalability, locking,  and threading?
</em> Unstructed Data
  * MapReduce is just Java</p>
<p><em>I can give a devleoper who knows nothing about HDFS a job to run (and
they have no appreciation of what they're doing.)</em></p>
<h2>Python</h2>
<p><strong>I've had long discussions with Java developers and I think it comes
down to a personality preference about why Python over Java.</strong>
Compiled vs. Scripting
With Java I have to compile. I compile to a JAR file, copy that to the
cluster, realize there's a problem, and then come back. </p>
<p>With scripting (python) it's fairly easy to edit-in-place.</p>
<ul>
<li>Python vs. Java</li>
<li>Compiled vs. scripts</li>
<li>Python libraries we all love</li>
<li>World-class libraries for data analysis, etc.</li>
<li>Integration with other things</li>
</ul>
<p><strong>Why Not Python?</strong></p>
<ul>
<li>
<p>You might be on your own in some cases with downloading, installing</p>
</li>
<li>
<p>Smaller community, almost no official support</p>
</li>
</ul>
<h2>Survey of Python Hadoop things</h2>
<p><strong>MrJob</strong>
Wrape Hadoop streaming that wraps the MapReduce processes (used and
maintained by Yelp). Well-documented and can run locally in Amazon
Elastic MapReduce (EMR),
or Hadoop.</p>
<p><strong>Pydoop</strong>
You can write MapReduce jobs in Python. Uses C++ Pipes which should be
faster than wrapping streaming.</p>
<p>See Clouderas blogpost guide to python frameworks for Hadoop</p>
<p><strong>Pig</strong>
Does data flow transformation. A higher level platform and language for
analyzing data that happens to run MapReduce underneath.
Book: Agile Data Science - not terribly useful but fun to read</p>
<p><strong>Snakebite</strong>
Big missing part: cannot write data.</p>
<p>(See Luigi from Spotify, see the Spotify open source projects)</p>
<p><strong>HBase</strong>
Not really there yet and has failed to gain community momentum. Java is
still King.</p>
<h2>The Future</h2>
<p><strong>Spark</strong>
Spark generally is a lot faster and easier to write than MapReduce. It provides a high-level API in Scala, Java, and Python that
makes parallel jobs easy to write. <strong>PySpark</strong> 
Some folks say that Spark is better than MapReduce
RDDs = Resilient Distributed Datasets
RDDs are kept in memory for the most part.
Spark does the computations in memory as opposed to MapReduce. But if
your dataset gets too large it writes to disk. Get get comfortable with
lambdas.</p>
<h2>Update, see the video of the talk below</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/g99U7c4jSNs" frameborder="0" allowfullscreen></iframe>
		</div>
	</div>
</div>

    </div>
  </div>
</div>
</body>
</html>